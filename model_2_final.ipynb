{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prudhvi0520/DataminingAssign1/blob/main/model_2_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HA22kaQdh7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56885e9-71f8-4997-ee0a-c7b67f4d66aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/Colab Notebooks/final/final/images.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/images\")\n",
        "zip_ref.close()\n",
        "!pip install livelossplot\n",
        "import os \n",
        "import glob\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "import tensorflow as tf\n",
        "from livelossplot.tf_keras import PlotLossesCallback\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ped4p7Ridh7b"
      },
      "outputs": [],
      "source": [
        "# understanding the dataset structure\n",
        "data_dir = '../images/images/'\n",
        "training_path = '/content/images/images/images/train/'\n",
        "validation_path = '/content/images/images/images/validation/'\n",
        "\n",
        "\n",
        "# Training classes\n",
        "print(\"--------- Training set description --------------\")\n",
        "print(\"S.No.\\tSample Count\\tClass\\t\")\n",
        "# Printing distribution of each class in training set\n",
        "for (i,expression) in enumerate(os.listdir(training_path)):\n",
        "    print(str(i+1)+ \"\\t\"+ str(len(os.listdir(training_path + expression))) + \"\\t\\t\" + expression )\n",
        "    \n",
        "\n",
        "# Validation classes\n",
        "print(\"\\n--------- Validation set description --------------\")\n",
        "print(\"S.No.\\tSample Count\\tClass\\t\")\n",
        "# Printing distribution of each class in training set\n",
        "for (i,expression) in enumerate(os.listdir(validation_path)):\n",
        "    print(str(i+1)+ \"\\t\"+ str(len(os.listdir(validation_path + expression))) + \"\\t\\t\" + expression )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT9fjJNkdh7b"
      },
      "outputs": [],
      "source": [
        "def create_dataset(main_path):\n",
        "    label_encoder = LabelEncoder()\n",
        "\n",
        "    df = {\"img\":[],\"img_class\":[]}\n",
        "    for class_names in os.listdir(main_path):\n",
        "        for img_path in glob.glob(f\"{main_path}/{class_names}/*\"):\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            img = cv2.resize(img, (48, 48))\n",
        "            img = img[..., np.newaxis] / 255.0         \n",
        "            df[\"img\"].append(img)\n",
        "            df[\"img_class\"].append(class_names)\n",
        "    df[\"img\"] = np.array(df[\"img\"], dtype='float32')\n",
        "    df[\"img_class\"] = label_encoder.fit_transform(df[\"img_class\"])\n",
        "    df[\"img_class\"] = tf.keras.utils.to_categorical(df[\"img_class\"])\n",
        "    return df[\"img\"],df[\"img_class\"]\n",
        "    \n",
        "training_dataset, training_labels = create_dataset(training_path[:len(training_path)-1])\n",
        "validation_dataset, validation_labels  = create_dataset(validation_path[:len(validation_path)-1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VueY_wHGdh7b"
      },
      "outputs": [],
      "source": [
        "# Defining constants \n",
        "BATCH_SIZE = 64\n",
        "img_channel = 3\n",
        "LEARNING_RATE = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWPBvW5fdh7c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "data_augmentation = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    fill_mode='reflect'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn3gRPKldh7d"
      },
      "source": [
        "<h3>Building Model</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQH7zfEVdh7d"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.layers import Dense,  Dropout,Flatten, Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1 - Convolution\n",
        "model.add(Conv2D(32, (3, 3),  input_shape=(48, 48, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "# 2nd Convolution layer\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "# Flattening\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully connected layer 1st layer\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "opt = Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yqpmppydh7f"
      },
      "outputs": [],
      "source": [
        "early_stopping=tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\",patience=3,mode=\"auto\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"best_weights.h5\",verbose=1,save_best_only=True,save_weights_only = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1y2TL3-dh7f"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    data_augmentation.flow(training_dataset, training_labels, batch_size=BATCH_SIZE),    \n",
        "    epochs=100,\n",
        "    callbacks=[PlotLossesCallback(),checkpoint, early_stopping],\n",
        "    validation_data=(validation_dataset,validation_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhaQhnVJdh7g"
      },
      "outputs": [],
      "source": [
        "# Save Model\n",
        "model.save(\"FacialExpressionModel.h5\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}